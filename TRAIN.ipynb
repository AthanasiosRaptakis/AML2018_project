{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from include.data import ObjectSegmentationDataset\n",
    "from include.hybrid_net import SegmentationModel\n",
    "\n",
    "from include.Utility_Functions import Validate_IOU,Validate_CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders for Training and Validation\n",
    "\n",
    "# It is important to normalise the Input images acording to\n",
    "# https://pytorch.org/docs/stable/torchvision/models.html\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[0.458, 0.438, 0.404],std=[0.266, 0.263, 0.278])])\n",
    "\n",
    "train_dataset = ObjectSegmentationDataset(\n",
    "    src_image_dir=\"/home/thanos/Desktop/Dataset/TRAIN/Data/\",\n",
    "    seg_image_dir=\"/home/thanos/Desktop/Dataset/TRAIN/Ground_Truth/\",\n",
    "    augment=True,\n",
    "    rescale=(255,255),\n",
    "    num_classes=21,\n",
    "    transform=transform)\n",
    "\n",
    "validation_dataset = ObjectSegmentationDataset(\n",
    "     src_image_dir=\"/home/thanos/Desktop/Dataset/VAL/Data/\",\n",
    "     seg_image_dir=\"/home/thanos/Desktop/Dataset/VAL/Ground_Truth/\",\n",
    "     augment=False,\n",
    "     rescale=False,\n",
    "     num_classes=21,\n",
    "     transform=transform)\n",
    "\n",
    "bs=10\n",
    "Train_Loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=4)\n",
    "VAL_Loader = torch.utils.data.DataLoader(validation_dataset, batch_size=15, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SegmentationModel(num_classes=20)\n",
    "filename=\"/home/thanos/Desktop/Dataset/MODELS/IOU_Best_params.pth\"\n",
    "checkpoint = torch.load(filename)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_IOU=0.0\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=20)\n",
    "learning_rate=0.001\n",
    "optimizer = optim.SGD([\n",
    "                {'params': net.vgg.parameters(), 'lr': 1e-3},\n",
    "                {'params': net.atr.parameters(), 'lr': 1e-2}\n",
    "            ], momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lambda1 = lambda epoch: 0.001*(+1.0 - (epoch / 10000.0) )**0.9 \n",
    "lambda2 = lambda epoch: 0.01*(+1.0 - (epoch / 10000.0) )**0.9 \n",
    "\n",
    "Scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2], last_epoch=-1)\n",
    "net.train()\n",
    "losses=[]\n",
    "IOU_Score=[]\n",
    "\n",
    "for epoch in range(10000):\n",
    "    \n",
    "    epoch_loss=0.0\n",
    "    \n",
    "    for data in Train_Loader:\n",
    "        src_img, _, seg_img, _ = data\n",
    "\n",
    "        Input = Variable(src_img).float().cuda()\n",
    "        Target = Variable(seg_img.long(), requires_grad=False).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        Output = net(Input)\n",
    "        \n",
    "\n",
    "        loss = loss_fn(Output, Target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss+=(loss.item())/10582.0\n",
    "\n",
    "            \n",
    "    # Checkpoint Network and Optimiser after each 25 epochs\n",
    "    if (epoch % 25==0) :\n",
    "        best_IOU,iou=Validate_IOU(net,optimizer,epoch,losses,IOU_Score,bs,learning_rate,VAL_Loader,best_IOU)\n",
    "    \n",
    "    losses.append(epoch_loss)\n",
    "    IOU_Score.append(iou)\n",
    "    Scheduler.step()\n",
    "    print(\"Epoch = {} , Epoch_Loss = {} , Best_IOU = {} \".format(epoch,epoch_loss,best_IOU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(IOU_Score)\n",
    "plt.title(\"IOU in Validation Dataset\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"IOU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
